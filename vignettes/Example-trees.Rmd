---
title: "Comparing trees with quartets"
author: "Martin R. Smith"
date: "30 November 2017"
output: rmarkdown::html_document
bibliography: ../inst/REFERENCES.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/systematic-biology.csl
vignette: >
  %\VignetteIndexEntry{Example Trees}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
require('ape')
require('SlowQuartet')
data('sq_trees')
n_tip <- 11
ref_tree <- sq_trees[[1]]
tip_colours <- Ternary::cbPalette15[-c(4, 7)] # Rm Tritanopia duplicates of 13 and 3
palette4 <- Ternary::cbPalette8[1:4]

rf_max <- (n_tip - 3) * 2

colplot <- function (tr, title=NULL, bold=NULL, ...) {
  tr$edge.length <- rep(1, dim(tr$edge)[1])
  font <- rep(1, length(tr$tip.label))
  if (!is.null(bold)) font[tr$tip.label %in% bold] <- 4
  plot(tr, tip.col=tip_colours[as.integer(tr$tip.label)], main=title, cex.main=0.8, font=font, y.lim=c(-3.5, n_tip), ...)
}

rfplot <- function (tr, title=NULL, highlight=NULL, ref=ref_tree, ...) {
  tree_dist <- phangorn::treedist(tr, ref)
  tree_pair <- lapply(list(tr, ref), ape::root, outgroup='1', resolve.root=FALSE)
  class(tree_pair) <- 'multiPhylo'
  topo_dist <- as.matrix(ape::dist.topo(tree_pair, method='PH85'))[2]
  
  colplot(tr, title, highlight, cex=0.8, ...)
  text_x <- par('usr')[2] * 0.5
  
  text(text_x, -0.5, 'Quartet:', cex=0.8, pos=2)
  text(text_x, -1.5, "RF:", cex=0.8, pos=2)
  text(text_x, -2.5, "Path:", cex=0.8, pos=2)
  text(text_x, -3.5, "SPR:", cex=0.8, pos=2)
  text(text_x, -0.5, paste0(MatchingQuartets(list(tr, ref))['d', 2], '/', choose(11,4)), cex=0.8, pos=4)
  text(text_x, -1.5, paste0(topo_dist, '/', rf_max), cex=0.8, pos=4)
  text(text_x, -2.5, paste0(signif(tree_dist[2], 3)), cex=0.8, pos=4)
  text(text_x, -3.5, paste0(phangorn::sprdist(tr, ref)[1]), cex=0.8, pos=4)
}

polyplot <- function (tr, title, highlight, ...) {
  tree_pair <- lapply(list(ref_tree, tr), ape::unroot)
  class(tree_pair) <- 'multiPhylo'
  partitions <- MatchingSplits(tree_pair)[, 2]
  
  colplot(tr, title, highlight, ...)
  x_mid <- par('usr')[2] * 0.62
  
  text(x_mid, -0.5, cex=0.8, pos=2, "Quartets contradicted")
  text(x_mid, -1.5, cex=0.8, pos=2, "Quartets unresolved")
  text(x_mid, -2.5, cex=0.8, pos=2, "Partitions contradicted")
  text(x_mid, -3.5, cex=0.8, pos=2, "Partitions unresolved")
  
  text(x_mid, -0.5, cex=0.8, pos=4, 
       paste0(MatchingQuartets(tree_pair)['d', 2], '/', choose(11,4)))
  text(x_mid, -1.5, cex=0.8, pos=4, 
       paste0(sum(MatchingQuartets(tree_pair)[c('r1','r2','u'), 2]), '/', choose(11,4)))
  # The below assumes that ref is bifurcated, so if a partition's in cf not ref it must contradict.
  text(x_mid, -2.5, cex=0.8, pos=4, paste0(partitions['cf_not_ref'], '/', rf_max / 2))
  text(x_mid, -3.5, cex=0.8, pos=4, paste0(partitions['ref_not_cf'] - partitions['cf_not_ref'], '/', rf_max / 2)) 
}

#par(mfrow=c(3,5), mar=rep(1.2, 4), cex.main=2/3, cex=0.9)
#for (i in seq_along(sq_trees)) colplot(sq_trees[[i]], names(sq_trees)[i])

```

## Tree distance metrics

A number of metrics area available to quantify the similarity between two undirected
topologies (i.e. unrooted trees with no edge lengths).

### Partition metric
The Robinson-Foulds (RF, 'partition' or 'symmetric difference') metric [@Robinson1981; @Steel1993]
adds the number of bipartitions that are present in tree A (but not tree B)
to the number of bipartitions present in tree B (but not tree A).  

It is most useful when the trees to be compared are very similar; it has
a low range of integer values, limiting its ability to distinguish between
trees [@Steel1993].

### SPR metric
The subtree pruning and regrafting (SPR) distance [@Penny1985] counts the 
number of SPR rearrangements necessary to transform Tree A into Tree B.

### Path difference metric
The length of a path from one tip to another in a tree is the number of edges
within the tree that must be crossed to navigate from one tip to the other.

Given two trees, is possible to calculate the difference in path length between
each pair of tips.

The path difference metric [@Steel1993] is the square root of the sum of squares
of each of these differences.

### Quartet metric
The quartet metric [@Estabrook1985] counts what proportion of four-taxon statements 
are true in both trees.

For any four tips A, B, C and D, a bipartition on a bifurcating tree will separate
tip A and either B, C or D from the other two tips.  That is to say, removing
all other tips from the tree will leave one of these three trees:

```{R Three four-taxon trees, echo=FALSE, cache=TRUE, fig.width=6, fig.asp=1/3, out.width='66%', fig.align='center',dev='svg'}
par(mfrow=c(1, 3), mar=c(0.5, 1, 0.5, 1), cex=1)
plot(ape::read.tree(text='((A, B), (C, D));'), tip.color=Ternary::cbPalette8[c(1, 4, 7, 5)], font=2)
plot(ape::read.tree(text='((A, C), (B, D));'), tip.color=Ternary::cbPalette8[c(1, 7, 4, 5)], font=2)
plot(ape::read.tree(text='((A, D), (C, B));'), tip.color=Ternary::cbPalette8[c(1, 5, 7, 4)], font=2)
```

Thus two of the random trees below share the quartet `(A, B), (C, D)`, whereas 
the third does not; these four tips are divided into `(A, D), (B, C)`.

```{R Plot a quartet, echo=FALSE, cache=TRUE, fig.asp=1.3/3, fig.width=6, out.width='80%', fig.align='center',dev='svg'}
par(mfrow=c(1, 3))
set.seed(7)
trees7 <- lapply(logical(3), function (X) {
    tr <- ape::rtree(7, br=NULL)
    tr$edge.length <- rep(1, 12)
    tr$tip.label <- LETTERS[1:7]
    tr
  })
PlotQuartet(trees7, LETTERS[1:4], cex=1.4, font=2)
```

There are $n\choose4$ groups of four taxa in a tree with $n$ tips;
for each of these groups, one of the three trees above will be consistent with
a given tree.  As such, two identical trees will have a quartet distance of
0, and a random pair of trees will have an expected ${n\choose{4}} / 3$
quartets in common. Because quartets are not independent of one another,
no pair of trees with six or more tips can have all $n\choose4$ quartets in
common [@Steel1993].

## Desired behaviour of tree distance metrics

The advantages of the Quartet distance [@Estabrook1985] over other tree distance
metrics [@Penny1985] are best illustrated by examining a set of example trees.

### Moving a single taxon

If trees differ only in the location of a single taxon, then the distance between
two trees should correspond to the distance that this taxon has been moved.

```{R Moving a single taxon, fig.asp=1.8/5, out.width='100%', fig.width=6, echo=FALSE, cache=TRUE, dev='svg'}
par(mfrow=c(1, 4), mar=rep(0.4, 4))
colplot(ref_tree,     'Reference tree', 1)
rfplot(sq_trees$move_one_near, 'Short move', 1)
rfplot(sq_trees$move_one_mid, 'Medium move', 1)
rfplot(sq_trees$move_one_far,   'Long move', 1)
```

The subtree pruning and regrafting (SPR) distance does not distinguish
between these trees, as they differ only in the placement of a single tip.
The Robinson-Foulds, path difference and quartet metrics, 
in contrast, recognize trees in which this tip has been 
moved further as more distant from the starting tree.

### Moving two taxa

Intuitively, moving a pair of tips on a tree should lead to higher tree distances
than moving a single tip.  In the case of a short move, the RF distance
does not differ whether one or two tips are moved.
For larger moves, however, the RF distance is _less_ when two tips are moved than
when a single tip is moved.  The path and quartet metrics perform as expected.

```{R Moving a cherry, echo=FALSE, cache=TRUE, fig.asp=1.6*3/3, fig.width=6, out.width='66%', fig.align='center', dev='svg'}
par(mfrow=c(3, 3), mar=c(2.4, 0.4, 0.4, 0.4), cex=1)

colplot(ref_tree, 'Reference tree')
rfplot(sq_trees$move_one_near, 'Short move 1', 1)
rfplot(sq_trees$move_two_near, 'Short move 2', 10:11)

colplot(ref_tree, 'Reference tree')
rfplot(sq_trees$move_one_mid, 'Medium move 1', 1)
rfplot(sq_trees$move_two_mid, 'Medium move 2', 10:11)

colplot(ref_tree, 'Reference tree')
rfplot(sq_trees$move_one_far,   'Long move 1', 1)
rfplot(sq_trees$move_two_far,   'Long move 2', 10:11)

```

### Maximum distance
A distance metric should distinguish slightly-perturbed trees from
random trees and those that are more different from the starting tree than 
expected by chance.

The worst-case scenario for the Robinson-Foulds metric involves the relocation
of a single taxon in a pectinate tree. As there are no partitions 
shared between the two trees, this situation generates the maximum
RF distance, and this trivially deformed tree is considered to be as distant
as a random tree.

```{R Pectinate to break RF, echo=FALSE, cache=TRUE, fig.asp=1.5/3, fig.width=6, out.width='90%', fig.align='center', dev='svg'}
par(mfrow=c(1, 3), mar=rep(0.4, 4))

pectinate_tree  <- ape::read.tree(text='(1, (2, (3, (4, (5, (6, (7, (8, (9, (10, 11))))))))));')
pectinate_unrooted <- ape::unroot(pectinate_tree)
pectinate_move1 <- ape::read.tree(text='(2, (3, (4, (5, (6, (7, (8, (9, (10, (11, 1))))))))));')
colplot(pectinate_tree, 'Pectinate tree')
rfplot(pectinate_move1,  'Move one taxon', 1, ref=pectinate_unrooted)
rfplot(sq_trees$random_tree,   'Random tree',   1, ref=pectinate_unrooted)
```

An advantage of the Quartet distance is that the normalized metric of a random
tree is $2/3$ [@Day1986; @Steel1993].  As such, trees that are more different than expected by chance
can be readily recognized, as their quartet distance will be greater than 
`r choose(n_tip, 4) * 2 / 3`/`r choose(n_tip, 4)`:

```{R Increasing distances, echo=FALSE, cache=TRUE, fig.asp=1.4/3, fig.width=6, out.width='90%', fig.align='center', dev='svg'}
par(mfrow=c(1, 3), mar=rep(0.4, 4))
colplot(ref_tree, 'Reference tree')
rfplot(sq_trees$random_tree,   'Random tree'     )
rfplot(sq_trees$opposite_tree, 'Maximum distance')
```

### Unresolved trees

Whereas the path distance and SPR metrics are only defined on bifurcating trees,
the quartet approach can be applied to trees that contain polytomies -- i.e. 
not every node is resolved as bifurcating.

Estabrook _et al_. [-@Estabrook1985] propose four measures of quartet distance to 
be applied where one or both trees contain polytomies, each of which evaluate
to the quartet distance if both trees are bifurcating.

For each of the $Q$ possible quartets, for a pair of trees that contain $n$ taxa, there are five possibilities [@Estabrook1985]. If the quartet is resolved in both trees, then the
quartets may be resolved in the same way ($s$) or differently ($d$).  Otherwise, 
the quartet may be resolved in tree 1 only ($r_1$), resolved in tree 2 only ($r_2$),
or unresolved in both trees ($u$).

Various assemblies of these measures provide four quartet measures for 
trees that contain polytomies:

* $(Q - d) / Q$ gives the number of quartets that do not conflict (_DNC_). 

* $s / Q$ counts the number of resolved quartets that explicitly agree (_EA_) in
both trees.

* $s / (s + d)$ counts, of the quartets that are resolved on both trees, what 
proportion agree -- the strict joint assertions (_SJA_) measure.

* $(s + u) / (s + d + u)$ counts, 
of the quartets that are either resolved or unresolved on both trees, what
proportion do not conflict -- the semi-strict joint assertions (_SSJA_) measure.

I propose a fifth metric, which is analogous to the 
Kullback-Leiber divergence [@Kullback1951]: it measures the amount of information
that must be supplied to change one tree into the other.

Here, a resolved quartet represents a single unit of information.
(Because quartets are not independent of one another,
the term "information" is not equivalent to the Shannon entropy.)

To navigate from one tree to another, one requires a list of quartets that must
be 'unpicked',
comprising the $d$ different quartets and the $r_1$ that are present only in
the starting tree, and a list of quartets that must be forged, comprising the
$d$ different quartets (which, having been removed, must now be added in their
correct configuration) and the $r_2$ quartets that are resolved only on the second
tree.  The normalized quartet divergence (_QD_) is given by 
$(2d + r_1 + r_2) / 2Q$.  (An equivalent calculation underpins the
partition metric.)

In bifurcating trees, these five measures evaluate to the same quantity
[@Estabrook1985].  


```{R Polytomies - same topology, echo=FALSE, cache=TRUE, fig.asp=1.6/3, fig.width=6, out.width='90%', fig.align='center', dev='svg'}
par(mfrow=c(1, 3), mar=c(0.4, 0.4, 0.4, 0.4))

colplot(ref_tree, 'Reference tree')
polyplot(sq_trees$collapse_one, 'One polytomy', 7:9)
polyplot(sq_trees$collapse_some, 'Much polytomy', 1:9)

```

In the above case, the underlying topology was unchanged except for the loss
of resolution.  We can also change the topology before we collpase nodes -- 
here we introduce the same loss of resolution in trees in which one tip
(tip 1) was moved:

```{R Polytomies 1, echo=FALSE, cache=TRUE, fig.asp=1.6/3, fig.width=6, out.width='90%', fig.align='center', dev='svg'}
par(mfrow=c(1, 3), mar=c(0.4, 0.4, 0.4, 0.4))

polyplot(sq_trees$move_one_mid, 'One moved, no polytomy', 1)
polyplot(sq_trees$m1mid_col1, 'One moved, one polytomy', 1)
polyplot(sq_trees$m1mid_colsome, 'One moved, much polytomy', 1)

```

And here again, collapsing nodes in the trees
in which two taxa (10 & 11) were moved:

```{R Polytomies 2, echo=FALSE, cache=TRUE, fig.asp=1.6/3, fig.width=6, out.width='90%', fig.align='center', dev='svg'}
par(mfrow=c(1, 3), mar=c(0.4, 0.4, 0.4, 0.4))

polyplot(sq_trees$move_two_mid,  'Two moved, no polytomy'  , 10:11)
polyplot(sq_trees$m2mid_col1,    'Two moved, one polytomy' , 10:11)
polyplot(sq_trees$m2mid_colsome, 'Two moved, much polytomy', 10:11)

```

## Plotting this data

We can visualize this data using a ternary diagram.

```{R Plot some trees, echo=FALSE, cache=TRUE, fig.asp=1/2, fig.width=8, out.width='100%', fig.align='center', dev='svg'}
library('Ternary')
par(mfrow=c(1, 2), mar=rep(0.3, 4))
collapse_perfect <- c('ref_tree', 'collapse_one', 'collapse_some')
collapse_move1   <- c('move_one_mid', 'm1mid_col1', 'm1mid_colsome')
collapse_move2   <- c('move_two_mid', 'm2mid_col1', 'm2mid_colsome')

TernaryPlot(' Consistent', ' Contradicted', 'Unresolved', lab.cex=0.8, lab.font=2,
            grid.lty='solid', col=rgb(0.9, 0.9, 0.9), grid.col='white', 
            axis.col=rgb(0.6, 0.6, 0.6), ticks.col=rgb(0.6, 0.6, 0.6),
            padding=0.1)
title(main="\nQuartet distances", cex.main=0.8)
HorizontalGrid()
AddToTernary(lines, list(c(1/3, 2/3, 0), c(0, 0, 1)), lty='dotted', col=cbPalette8[8], lwd=2)

pchs <- c(1, 4, 3)
quartet_distances <- QuartetPoints(sq_trees)
TernaryPoints(quartet_distances[collapse_perfect, ], col=cbPalette8[2], pch=pchs, cex=1.5)
TernaryLines (quartet_distances[collapse_perfect, ], col=cbPalette8[2])
TernaryPoints(quartet_distances[collapse_move1, ],   col=cbPalette8[3], pch=pchs, cex=1.5)
TernaryLines (quartet_distances[collapse_move1, ],   col=cbPalette8[3])
TernaryPoints(quartet_distances[collapse_move2, ],   col=cbPalette8[4], pch=pchs, cex=1.5)
TernaryLines (quartet_distances[collapse_move2, ],   col=cbPalette8[4])
legend('bottomright', 
       lty=rep('dotted', 2),
       lwd=c(1, 2),
       col=c('grey', cbPalette8[8]),
       legend=c("Lines of equal RF distance\n/ quartet divergence", 'Mean distance of random tree'), 
       cex=0.8, bty='n')
arrows(sqrt(3/4) * 0.5, 0.5, sqrt(3/4) * 0.8, 0.5, length=0.1)
text  (sqrt(3/4) * 0.65, 0.5, pos=3, 'Decreasing resolution', cex=0.8)

arrows(sqrt(3/4) * 0.98, 0.40, sqrt(3/4) * 0.98, 0.20, length=0.1)
text  (sqrt(3/4) * 1.01, 0.30, pos=3, 'Increasing divergence', cex=0.8, srt=270)

TernaryPlot(' Consistent', ' Contradicted', 'Unresolved', lab.cex=0.8,
            grid.lty='solid', col=rgb(0.9, 0.9, 0.9), grid.col='white', 
            axis.col=rgb(0.6, 0.6, 0.6), ticks.col=rgb(0.6, 0.6, 0.6),
            padding=0.1)
title(main="\nPartition distances", cex.main=0.8)

HorizontalGrid()
partition_distances <- SplitsPoints(sq_trees)
TernaryPoints(partition_distances[collapse_perfect, ], col=cbPalette8[2], pch=pchs, cex=1.5)
TernaryLines (partition_distances[collapse_perfect, ], col=cbPalette8[2])
TernaryPoints(partition_distances[collapse_move1, ],   col=cbPalette8[3], pch=pchs, cex=1.5)
TernaryLines (partition_distances[collapse_move1, ],   col=cbPalette8[3])
TernaryPoints(partition_distances[collapse_move2, ],   col=cbPalette8[4], pch=pchs, cex=1.5)
TernaryLines (partition_distances[collapse_move2, ],   col=cbPalette8[4])

arrows(sqrt(3/4) * 0.98, 0.40, sqrt(3/4) * 0.98, 0.20, length=0.1)
text  (sqrt(3/4) * 1.01, 0.30, pos=3, 'Increasing RF distance', cex=0.8, srt=270)

legend('bottom', cex=0.8, bty='n',
       pch=20, pt.cex=1.5, col=cbPalette8[2:4],
       title='Move:',
       legend=c('No tips', 'One tip', 'Two tips'))

legend('bottomright', cex=0.8, bty='n',
       pch=pchs, pt.cex=1.5, col='black',
       title='Collapse:',
       legend=c('No nodes', 'One node', 'Many nodes')
)
```

## Using ternary diagrams to inform tree reconstruction techniques
This means of visualization provides a helpful way to understand how effective
different methods of phylogenetic reconstruction are on particular trees.

Here I have taken a representative dataset simulated from a 22-tip reference tree [@Congreve2016],
and analysed the dataset in TNT v1.5 [@Goloboff2016] under equal weights parsimony 
and implied weights (with concavity constants of 1, 3 and 5), and in MrBayes v3.2.2
[@Huelsenbeck2001] using the Markov K model [@Lewis2001].

For each parsimony analysis, I recorded a strict consensus of (i) all optimal trees;
and (ii) all trees found whose score was within $x$ of the optimal tree, with
$x$ taking the values 1..20 under equal weights and a logarithmically uniform 
series of values, $0.73^{19..0}$ (= `r signif(0.73^(19:0), 2)`), under 
implied weights.

For each Bayesian analysis, I resolved all nodes whose posterior probability was 
at least $x$, at 20 uniformly spaced values of $x$ from 0.5 to 1.0.

These trees represent a progressive loss of resolution (precision) from the
optimal tree, allowing an exploration of the relationship between precision and
accuracy.  In each plot,resolution decreases from left to right. 

Congreve and Lamsdell [-@Congreve2016] argue that equal weighting is the optimal
superior method because it resolves the fewest incorrect bipartitions -- that 
is, its most-resolved tree is the closest to the top-right side of the ternary diagram.
By this measure, all methods are improved by collapsing nodes until none remain.

On the view advocated here, the optimal tree is the one that has the greatest
_y_ coordinate, and thus diverges least from the reference tree.  By this measure, 
collapsing the least-supported nodes in implied-weighting trees leads to an
increase in tree quality, as
predicted by Goloboff [-@Goloboff1995]: nodes with low support are likely to 
be incorrect. Collapsing better-supported nodes, however, reduces tree quality: nodes
with high support are likely to be correct.  The same pattern holds here for
Bayesian trees, though in equal weights it seems that the maximum resolution 
attainable is already on the descending part of the curve.  

```{R Congreve Lamsdell tree, echo=FALSE, cache=TRUE, fig.asp=1/3, fig.width=9, out.width='100%', fig.align='center'}
par(mfrow=c(1, 3), mar=rep(0.3, 4))
data(clPartitions)
data(clQuartets)

PCH_MK <- 1   # circle
PCH_EQ <- 61  #'='
PCH_XX <- 183 #'.'
PCH_IW <- 2   #triup
PCH_IC <- 17  #triupfilled

COL_MK <- cbPalette8[4]
COL_EQ <- cbPalette8[2]
COL_1  <- cbPalette15[6]
COL_2  <- cbPalette15[7]
COL_3  <- cbPalette15[8]
COL_5  <- cbPalette15[4]
COL10  <- cbPalette15[5]
COL_C  <- cbPalette15[12]

Quartet2Ternary <- function (item) clQuartets[[item]][c('s', 'd', 'r2'), , TREE]
  
Split2Ternary <- function (item) {
  itemData <- clPartitions[[item]][, , TREE]
  rbind(itemData['cf_and_ref', ],
        itemData['cf_not_ref', ],
        itemData['ref', ] - itemData['cf', ])
}

TernaryQuarts<-function(Func=Quartet2Ternary, zoom=1, padding=0.1) {
  
  xLim <- c(0, sqrt(3/4)/zoom)
  yLim <- c(0.5-(1/zoom), 0.5)
  lab <- if (zoom == 1) c('  Same', ' Different', 'Unresolved') else rep('', 3)
  
  TernaryPlot(lab[1], lab[2], lab[3], lab.cex=0.8, lab.font=2,
              grid.lty='solid', col=rgb(0.9, 0.9, 0.9), grid.col='white', 
              grid.lines=7,  axis.labels = 
                if(zoom==1) round(seq(0, choose(22, 4), length.out=8), 0)
                else FALSE,
              axis.col=rgb(0.6, 0.6, 0.6), ticks.col=rgb(0.6, 0.6, 0.6),
              padding=padding, xlim=xLim, ylim=yLim)
  HorizontalGrid()
  AddToTernary(lines, list(c(1/3, 2/3, 0), c(0, 0, 1)), lty='dotted', col=cbPalette8[8], lwd=2)
  
  JoinTheDots(Func('implied10'), col=COL10,  pch=PCH_XX, cex=1.1)
  JoinTheDots(Func('implied5'), col=COL_5,  pch=PCH_IW, cex=1.1)
  JoinTheDots(Func('implied3'), col=COL_3,  pch=PCH_XX, cex=1.1)
  JoinTheDots(Func('implied2'), col=COL_2,  pch=PCH_XX, cex=1.1)
  JoinTheDots(Func('implied1'), col=COL_1,  pch=PCH_IW, cex=1.1)
  JoinTheDots(Func('impliedC'), col=COL_C,  pch=PCH_IC, cex=1.1)
  JoinTheDots(Func('equal'   ), col=COL_EQ, pch=PCH_EQ, cex=1.1)
  JoinTheDots(Func('markov'  ), col=COL_MK, pch=PCH_MK, cex=1.1)
}


TREE <- 2
TernaryQuarts()
title(main="\nQuartets", cex.main=0.8)
arrows(sqrt(3/4) * 0.5, 0.5, sqrt(3/4) * 0.8, 0.5, length=0.1)
text  (sqrt(3/4) * 0.65, 0.5, pos=3, 'Decreasing resolution', cex=0.8)

arrows(sqrt(3/4) * 0.98, 0.40, sqrt(3/4) * 0.98, 0.20, length=0.1)
text  (sqrt(3/4) * 1.01, 0.30, pos=3, 'Increasing divergence', cex=0.8, srt=270)

TernaryQuarts(zoom=6, padding=0.01)

TernaryPlot('Same', 'Different', 'Unresolved', lab.cex=0.8,
            grid.lty='solid', col=rgb(0.9, 0.9, 0.9), grid.col='white', 
            axis.col=rgb(0.6, 0.6, 0.6), ticks.col=rgb(0.6, 0.6, 0.6),
            padding=0.1, axis.labels = round(seq(0, 20, length.out=11), 0))
title(main="\nPartitions", cex.main=0.8)

HorizontalGrid()
partition_distances <- SplitsPoints(sq_trees)

JoinTheDots(Split2Ternary('markov'  ), col=COL_MK, pch=PCH_MK, cex=1.1)
JoinTheDots(Split2Ternary('equal'   ), col=COL_EQ, pch=PCH_EQ, cex=1.1)
JoinTheDots(Split2Ternary('implied5'), col=COL_5, pch=PCH_IW, cex=1.1)
JoinTheDots(Split2Ternary('implied5'), col=COL_5, pch=PCH_IW, cex=1.1)
JoinTheDots(Split2Ternary('implied3'), col=COL_3, pch=PCH_IW, cex=1.1)
JoinTheDots(Split2Ternary('implied1'), col=COL_1, pch=PCH_IW, cex=1.1)

arrows(sqrt(3/4) * 0.98, 0.40, sqrt(3/4) * 0.98, 0.20, length=0.1)
text  (sqrt(3/4) * 1.01, 0.30, pos=3, 'Increasing RF distance', cex=0.8, srt=270)

AddLegend <- function(pos='bottomright')
  legend(pos, cex=0.8, bty='n',
         lty=1,
         pch=c(PCH_MK, PCH_EQ, PCH_XX, PCH_IW, PCH_XX, PCH_XX, PCH_IW, PCH_IC), pt.cex=1.1,
         col=c(COL_MK, COL_EQ, COL10, COL_5, COL_3, COL_2, COL_1, COL_C),
         legend=c('Markov', 'Equal weights', paste0('Implied, k=', c(10, 5, 3, 2, 1, '2..10')))
  )
AddLegend()

```

### Quartet metric
We can also examine the situation if we average across all 100 trees:

```{R CL trees - quartets, echo=FALSE, cache=TRUE, fig.asp=1/2, fig.width=9, out.width='100%', fig.align='center', dev='svg'}
TREES <- 1:100
# # There is essentially no difference if we exclude datasets with a low CI.
# # The lines just slide up a little.
# data(clCI)
# TREES <- TREES[clCI > 0.26]

par(mfrow=c(1, 2), mar=rep(0.3, 4))
AverageQuarts <- function (item) apply(clQuartets[[item]][c('s', 'd', 'r2'), , TREES], 2, rowMeans)
TernaryQuarts(AverageQuarts)
TernaryQuarts(Func=AverageQuarts, zoom=3.5, padding=0.01)
AddLegend('topright')

```

How good is each tree?

```{R Quartet divergence for each point, echo=FALSE, fig.width=8}
divergences <- vapply(clQuartets, function (item) QuartetDivergence(apply(item[, , TREES], 2, rowMeans)), double(21))
rownames(divergences) <- c('Optimal', paste0('Subopt. ', 1:20))
colnames(divergences) <- c('Mk', 'EqlW', paste0('IW', c(1:3, 5, 10, 'cons')))
round(divergences, 4)
```

Which reduction is the best?

```{R Quartet divergence - optimals, echo=FALSE}
apply(divergences, 2, which.max) - 1
```

The only analyses to produce significantly different ($p = 0.01$) results from implied weights
(at $k = 3$) or Bayesian are:

```{R Quartet divergence - significance, echo=FALSE}
treeBests <- vapply(clQuartets, function (item) apply(apply(item[, , TREES], 2, QuartetDivergence), 1, max), double(length(TREES)))
tests <- apply(treeBests, 2, function (column) t.test(column, treeBests[, 'implied3'])$p.value)
names(tests[tests < 0.01])
```

###Partition metric

```{R CL trees - partitions, echo=FALSE, cache=TRUE, fig.asp=1, fig.width=6, out.width='60%', fig.align='center', dev='svg'}
AverageSplits <- function (item) {
  itemData <- apply(clPartitions[[item]][, , TREES], 2, rowMeans)
  rbind(itemData['cf_and_ref', ],
        itemData['cf_not_ref', ],
        itemData['ref', ] - itemData['cf', ])
}

TernaryPlot('Same', 'Different', 'Unresolved', lab.cex=0.8,
            grid.lty='solid', col=rgb(0.9, 0.9, 0.9), grid.col='white', 
            axis.col=rgb(0.6, 0.6, 0.6), ticks.col=rgb(0.6, 0.6, 0.6),
            padding=0.1, axis.labels = round(seq(0, 20, length.out=11), 0))
title(main="\nPartitions", cex.main=0.8)

HorizontalGrid()
partition_distances <- SplitsPoints(sq_trees)

JoinTheDots(AverageSplits('impliedC'), col=COL_C, pch=PCH_IC, cex=1.1)
JoinTheDots(AverageSplits('implied10'), col=COL10, pch=PCH_XX, cex=1.1)
JoinTheDots(AverageSplits('implied5'), col=COL_5, pch=PCH_IW, cex=1.1)
JoinTheDots(AverageSplits('implied3'), col=COL_3, pch=PCH_XX, cex=1.1)
JoinTheDots(AverageSplits('implied2'), col=COL_2, pch=PCH_XX, cex=1.1)
JoinTheDots(AverageSplits('implied1'), col=COL_1, pch=PCH_IW, cex=1.1)
JoinTheDots(AverageSplits('equal'   ), col=COL_EQ, pch=PCH_EQ, cex=1.1)
JoinTheDots(AverageSplits('markov'  ), col=COL_MK, pch=PCH_MK, cex=1.1)

arrows(sqrt(3/4) * 0.5, 0.5, sqrt(3/4) * 0.8, 0.5, length=0.1)
text  (sqrt(3/4) * 0.65, 0.5, pos=3, 'Decreasing resolution', cex=0.8)
arrows(sqrt(3/4) * 0.98, 0.40, sqrt(3/4) * 0.98, 0.20, length=0.1)
text  (sqrt(3/4) * 1.01, 0.30, pos=3, 'Increasing RF distance', cex=0.8, srt=270)
AddLegend()
```

How good is each tree?

```{R Partition divergence for each point, echo=FALSE, fig.width=8}
rf_dists <- vapply(clPartitions, function (item) apply(item['RF_dist', , TREES], 1, mean), double(21))
rownames(rf_dists) <- c('Optimal', paste0('Subopt. ', 1:20))
round(rf_dists, 4)
```

Which reduction is the best?

```{R Partition divergence - best, echo=FALSE}
apply(rf_dists, 2, which.min) - 1
```

Do different values of k produce significantly different results ($p = 0.01$)?

```{R Partition significance, echo=FALSE}
treeBests <- vapply(clPartitions, function (item) apply(item['RF_dist', , ], 2, min), double(length(TREES)))
tests <- apply(treeBests, 2, function (column) t.test(column, treeBests[, 'impliedC'])$p.value)
names(tests[tests < 0.01])
```

And partitions for comparison:

Of the concavity constants chosen for implied weights, $k=1$ is strikingly worse
than other values, reflecting the extreme philosophy it represents. 
On one view, nodes that are recovered by a range of concavity constants are likely
to be correct [@Goloboff2008].  Excluding low concavity constants from this range
[cf. @Smith2015] seems to be justified.

It is interesting to note that, for this particular dataset, Bayesian methods
are outperformed by parsimony methods.

<!--
### More discussion [To re-write]

It is sometimes of interest to establish which of a selection of trees is most 
instructive about the reference tree.  How this might be done is not
necessarily intuitive: there is often a tension between accuracy and precision.
That is to say, should a tree that is perfectly accurate but poorly resolved be
preferred to a tree that is partially inaccurate but completely resolved?

One approach to this trade-off is to seek the tree of minimum entropy: that is,
the tree that diverges least  from the reference
tree.

A tree that is perfectly resolved, yet contains some incorrect nodes, delivers 
more information -- but also more misinformation -- than a poorly-resolved (yet
accurate) tree. The Kullback-Leibler divergence is the sum of the information
that is present in the reference tree, but not the candidate tree, plus the 
(mis)information that is present in the candidate tree but not the reference tree.

Put simply, to get from the candidate tree to the reference tree, one must be told
which groupings in the candidate tree are incorrect, and which groupings should be 
added in their place.

In the above instances, adding the number of quartets or partitions present in 
one but not both of the reference and candidate trees generates a measure analogous
to the Kullback-Leibler divergence.
The measure is only analogous because neither the number of 
quartets nor the number of partitions corresponds to a specific quantity of
information / entropy.  

Counting only the number of partitions fails to capture the fact that 
different partitions have different inherent information contents. 
A partition separating taxa  into two evenly-sized bipartitions is consistent 
with fewer trees than a partition separating two taxa from all others, thus has
a higher (Shannon) information content.

In contrast, each quartet specifies that one of three possibilities exists on a 
tree, so every quartet statement has the same information content: one trit (= 
$log(3) / log(2)$ bits).  The problem is that quartet statements (and partition
statements) are not independent from one another, meaning that the quartet 
metric is at best a rough approximation of the Kullback-Leibler divergence.

This important caveat notwithstanding, the quartet distance provides a promising
(if not statistically satisfying) approach to measuring the divergence between
two separate trees.  On the ternary diagrams presented above, points of equal 
divergence are connected by horizontal lines:  That is to say, any tree on a given 
contour is equally informative about the reference tree.
-->

## Conclusion

The quartet divergence is the only available metric of tree distance that fullfils 
all of the following desiderata:

 - Allocates trees higher distances if a clade moves greater distances
 - Allocates trees higher distances if a the clade that is moved is larger
 - Distinguishes contradicted from unresolved information in trees that 
   are not fully bifurcating (resolved)
 - Identifies pairs of trees that are more random than expected by chance
 - Does not reach its maximum value after relatively trivial rearrangements
 
 The quartet divergence is proposed as a measure of how similar two trees are,
 taking into account the inevitable trade-off between precision and accuracy.

## References
